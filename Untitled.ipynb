{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03ed832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8250f06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>Operating result for the 12-month period decre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>HELSINKI Thomson Financial - Shares in Cargote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     According to Gran , the company has no plans t...\n",
       "1     For the last quarter of 2010 , Componenta 's n...\n",
       "2     In the third quarter of 2010 , net sales incre...\n",
       "3     Operating profit rose to EUR 13.1 mn from EUR ...\n",
       "4     Operating profit totalled EUR 21.1 mn , up fro...\n",
       "...                                                 ...\n",
       "2259  Operating result for the 12-month period decre...\n",
       "2260  HELSINKI Thomson Financial - Shares in Cargote...\n",
       "2261  LONDON MarketWatch -- Share prices ended lower...\n",
       "2262  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "2263  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[2264 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the dataset\n",
    "Data_analytics = pd.read_table(\"C:\\\\Users\\\\kvdiv\\\\OneDrive\\\\Desktop\\\\Data Analytics\\\\FinancialPhraseBank-v1.0\\\\FinancialPhraseBank\\\\Sentences_AllAgree.txt\",encoding = \"ISO-8859-1\",header=None)\n",
    "Data_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f97a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>Operating result for the 12-month period decre...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>HELSINKI Thomson Financial - Shares in Cargote...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0         1\n",
       "0     According to Gran , the company has no plans t...   neutral\n",
       "1     For the last quarter of 2010 , Componenta 's n...  positive\n",
       "2     In the third quarter of 2010 , net sales incre...  positive\n",
       "3     Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
       "4     Operating profit totalled EUR 21.1 mn , up fro...  positive\n",
       "...                                                 ...       ...\n",
       "2259  Operating result for the 12-month period decre...  negative\n",
       "2260  HELSINKI Thomson Financial - Shares in Cargote...  negative\n",
       "2261  LONDON MarketWatch -- Share prices ended lower...  negative\n",
       "2262  Operating profit fell to EUR 35.4 mn from EUR ...  negative\n",
       "2263  Sales in Finland decreased by 10.5 % in Januar...  negative\n",
       "\n",
       "[2264 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coursework=pd.DataFrame(Data_analytics)\n",
    "coursework=coursework[0].str.split('@',expand=True)\n",
    "coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56a31c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       According to Gran , the company has no plans t...\n",
       "1       For the last quarter of 2010 , Componenta 's n...\n",
       "2       In the third quarter of 2010 , net sales incre...\n",
       "3       Operating profit rose to EUR 13.1 mn from EUR ...\n",
       "4       Operating profit totalled EUR 21.1 mn , up fro...\n",
       "                              ...                        \n",
       "2259    Operating result for the 12-month period decre...\n",
       "2260    HELSINKI Thomson Financial - Shares in Cargote...\n",
       "2261    LONDON MarketWatch -- Share prices ended lower...\n",
       "2262    Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "2263    Sales in Finland decreased by 10.5 % in Januar...\n",
       "Name: 0, Length: 2264, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coursework[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db7a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(coursework[0],coursework[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c153f14",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to resolve any data file that matches '['**train*']' at C:\\Users\\kvdiv\\OneDrive\\Desktop\\Data Analytics\\FinancialPhraseBank-v1.0 with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-939100585011>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# The data is already divided into training and test sets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load the training set:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m train_coursework = load_dataset(\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"FinancialPhraseBank-v1.0\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Sentences_AllAgree\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[1;31m# Create a dataset builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1660\u001b[1;33m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[0;32m   1661\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[0;32m   1487\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         ).get_module()\n\u001b[0;32m   1165\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m         return LocalDatasetModuleFactoryWithoutScript(\n\u001b[0m\u001b[0;32m   1167\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         ).get_module()\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mget_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0msanitize_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mget_patterns_locally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m         )\n\u001b[1;32m--> 744\u001b[1;33m         data_files = DataFilesDict.from_local_or_remote(\n\u001b[0m\u001b[0;32m    745\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_extensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mALL_ALLOWED_EXTENSIONS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mfrom_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns_for_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             out[key] = (\n\u001b[1;32m--> 576\u001b[1;33m                 DataFilesList.from_local_or_remote(\n\u001b[0m\u001b[0;32m    577\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mfrom_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    542\u001b[0m     ) -> \"DataFilesList\":\n\u001b[0;32m    543\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m         \u001b[0mdata_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_patterns_locally_or_by_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_extensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[0morigin_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_origin_metadata_locally_or_by_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mresolve_patterns_locally_or_by_urls\u001b[1;34m(base_path, patterns, allowed_extensions)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to resolve any data file that matches '['**train*']' at C:\\Users\\kvdiv\\OneDrive\\Desktop\\Data Analytics\\FinancialPhraseBank-v1.0 with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./FinancialPhraseBank-v1.0\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_coursework = load_dataset(\n",
    "    \"FinancialPhraseBank-v1.0\",\n",
    "    name=\"Sentences_AllAgree\",\n",
    "    split=\"train\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,encoding ='encoding'\n",
    ")\n",
    "print(f\"Training dataset with {len(train_coursework)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be756c12",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to resolve any data file that matches '['**train*']' at C:\\Users\\kvdiv\\OneDrive\\Desktop\\Data Analytics\\FinancialPhraseBank-v1.0 with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-65d443802c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# The data is already divided into training and test sets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load the training set:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m test_coursework = load_dataset(\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"FinancialPhraseBank-v1.0\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Sentences_AllAgree\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[1;31m# Create a dataset builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1660\u001b[1;33m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[0;32m   1661\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[0;32m   1487\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         ).get_module()\n\u001b[0;32m   1165\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m         return LocalDatasetModuleFactoryWithoutScript(\n\u001b[0m\u001b[0;32m   1167\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         ).get_module()\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mget_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0msanitize_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mget_patterns_locally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m         )\n\u001b[1;32m--> 744\u001b[1;33m         data_files = DataFilesDict.from_local_or_remote(\n\u001b[0m\u001b[0;32m    745\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_extensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mALL_ALLOWED_EXTENSIONS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mfrom_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns_for_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             out[key] = (\n\u001b[1;32m--> 576\u001b[1;33m                 DataFilesList.from_local_or_remote(\n\u001b[0m\u001b[0;32m    577\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mfrom_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    542\u001b[0m     ) -> \"DataFilesList\":\n\u001b[0;32m    543\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m         \u001b[0mdata_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_patterns_locally_or_by_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_extensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[0morigin_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_origin_metadata_locally_or_by_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mresolve_patterns_locally_or_by_urls\u001b[1;34m(base_path, patterns, allowed_extensions)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to resolve any data file that matches '['**train*']' at C:\\Users\\kvdiv\\OneDrive\\Desktop\\Data Analytics\\FinancialPhraseBank-v1.0 with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./FinancialPhraseBank-v1.0\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "test_coursework = load_dataset(\n",
    "    \"FinancialPhraseBank-v1.0\",\n",
    "    name=\"Sentences_AllAgree\",\n",
    "    split=\"test\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,encoding='ISO-8859-1'\n",
    ")\n",
    "print(f\"Testing dataset with {len(test_coursework)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4760899",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_coursework' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-665b4129e63e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_coursework\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_coursework' is not defined"
     ]
    }
   ],
   "source": [
    "train_coursework [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccdd5658",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_coursework' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-534444f3ce17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Put the data into lists ready for the next steps...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_coursework\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_coursework\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_coursework' is not defined"
     ]
    }
   ],
   "source": [
    "# Put the data into lists ready for the next steps...\n",
    "train_text = [sample['text'] for sample in train_coursework]\n",
    "train_labels = [sample['labels'] for sample in train_coursework]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2db4d10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_coursework' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5cbc83b9164e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_coursework\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_coursework\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_coursework' is not defined"
     ]
    }
   ],
   "source": [
    "test_text = [sample['text'] for sample in test_coursework]\n",
    "test_labels = [sample['label'] for sample in test_coursework]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "text = vectorizer.fit_transform(coursework[0])\n",
    "labels = coursework[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=coursework, columns=['text', 'labels'])\n",
    "\n",
    "# Remove any blank reviews\n",
    "df = df[df[\"labels\"].notnull()]\n",
    "\n",
    "# shuffle the dataset for later.\n",
    "# Note this isn't necessary (the dataset is shuffled again before used), \n",
    "# but is good practice.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "labels = vectorizer.fit_transform(coursework[1])\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# select the 200 features that have the strongest correlation to a class from the\n",
    "# remaining 308 features.\n",
    "selected_features = \\\n",
    "SelectKBest(chi2, k=200).fit(text, labels).get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0, vocabulary=selected_features)\n",
    "label = vectorizer.fit_transform(coursework[1])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f386de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reprlib\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "print(f'Vocabulary size = {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af40eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b2834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "acc = accuracy_score(test_label, y_test_pred)\n",
    "print(f'Accuracy = {acc}')\n",
    "\n",
    "prec = precision_score(test_label, y_test_pred, average='macro')\n",
    "print(f'Precision (macro average) = {prec}')\n",
    "\n",
    "rec = recall_score(test_label, y_test_pred, average='macro')\n",
    "print(f'Recall (macro average) = {rec}')\n",
    "\n",
    "f1 = f1_score(test_label, y_test_pred, average='macro')\n",
    "print(f'F1 score (macro average) = {f1}')\n",
    "\n",
    "# We can get all of these with a per-class breakdown using classification_report:\n",
    "print(classification_report(test_label, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "classifier = rfc()\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_estimators':stats.randint(10,300),\n",
    "    'criterion':['gini','entropy'],\n",
    "    'min_samples_split':stats.randint(2,9),\n",
    "    'bootstrap':[True,False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(classifier, hyperparameters, n_iter=65, n_jobs=4)\n",
    "\n",
    "random_search.fit(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ba297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
